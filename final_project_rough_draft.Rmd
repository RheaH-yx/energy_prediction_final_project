---
title: "final_project_code"
output: html_document
date: "2023-11-28"
---

# Introduction:
The utilization of energy by household appliances represents a significant portion of overall energy consumption. Understanding and predicting this usage is vital for energy conservation and efficiency efforts. Our project aims to analyze a dataset encompassing temperature and humidity readings from a wireless sensor network, weather data from a nearby airport station, and energy consumption records of lighting fixtures, to build a predictive model for appliance energy use.

# Data Description:
Link to dataset: https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction 
In 2017, a group of researchers from the Thermal Engineering and Combustion Laboratory at University of Mons in Belgium published a dataset about the energy consumption of home appliances, such as televisions and fridge, in a newly constructed low energy house. 
The dataset is recorded at a 10 min interval for about 4.5 months, and it consists of 19735 instances and 31 features, which are summarized in the table in the Appendix. 
The dataset is already split into a training set (14803 instances, 75% of the total data) and a testing set (4932), which we will directly use for the subsequent model development and evaluation. 


“Appliances energy consumption” will be used as the target variable for the regression analysis.


# 1: Explanatory Data Analysis and Feature Selection
## (1) Data visualization for potentially correlated variables

First, let's read in the training set (75% of the original dataset) and testing set (25%), which is already split by the data provider. 
```{r}
library(dplyr)
train_df <- read.csv("training.csv")
test_df <- read.csv("testing.csv")
# View(train_df)

# datetime objects can't be used for linear regression models, let's remove it
train_df <- subset(train_df, select = -date)
test_df <- subset(test_df, select = -date)

names(train_df)
```
Variables starting with T represent temperatures measured from different rooms of the hour, and variables starting with RH measured humidity. Let's plot a pairplot for each of these two groups of variables to see if there are high correlation within each group. This step is to remove any correlated variables so as to avoid redundant information and prevent . Just a note that for the subsequent feature selection, we will only use train_df.


```{r}
# install.packages("psych")
library(psych)

temp_vars <- train_df[, grepl("^T", names(train_df))]
humidity_vars <- train_df[, grepl("^RH", names(train_df))]

# pairplot for temperature
pairs.panels(temp_vars, 
             gap=0.5, 
             bg=c("red", "yellow", "blue")[unclass(temp_vars$T1)],
             pch=21, 
             lm=TRUE)

# pairplot for humidity
pairs.panels(humidity_vars, 
             gap=0.5, 
             bg=c("red", "yellow", "blue")[unclass(humidity_vars$RH_1)],
             pch=21, 
             lm=TRUE)
```
The first image of temperature variables seem to have strong positive correlations with each other, as indicated by several high correlation coefficients (values close to 1).For example, T_out (Temperature outside from Chievres weather station) and T_6 (Temperature outside the north side of the building) have a correlation of 0.97, T7 (Termperature in ironing room) and T9 (Temperature in parents room) have a correlation of 0.94, T5 (Temperature in bathroom) and T9 have a correlation of 0.9, T3 (Temperature in laundry room area) and T9 have a correlation of 0.90. We may consider dropping T_out and T_9. For the correlation plot of humidities in different rooms, there isn't very high correlation value, so we'll keep all of them for now. 

```{r}
train_df <- train_df %>% select(-T_out, -T9)
```


In the second image, the humidity variables show less correlation among variables, so we'll also leave it unchanged. 


## (2) Best Subset Selection
```{r}
library(leaps)
regfit.full <- regsubsets(Appliances ~ ., train_df, , nvmax = 35)
reg.summary <- summary(regfit.full)
```

```{r}
plot(reg.summary$adjr2, xlab = "Number of Variables",
     ylab = "Adjusted RSq", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", cex = 2, pch=20)

plot(reg.summary$rss, xlab = "Number of Variables",
     ylab = "RSS", type = "l")
points(which.min(reg.summary$rss), reg.summary$rss[which.min(reg.summary$rss)], col = "red", cex = 2, pch=20)

plot(reg.summary$cp, xlab = "Number of Variables",
     ylab = "C_p", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch=20)

plot(reg.summary$bic, xlab = "Number of Variables",
     ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch=20)
```
BIC shows that the best number of variable is 19. Let's see the coefficient estimate for this model

```{r}
coef(regfit.full, 19)
```


# Model Development: Linear Regression

Before developing the model, we select the best 19 variables that we obtained from Best Subset selection. Then we converted all the string variable to factor so that they can fit into linear regression. 

```{r}
## extract the variables for future model develepment and testing
best_vars <- names(coef(regfit.full, 19))
best_vars <- best_vars[best_vars != "(Intercept)"]
best_vars <- best_vars[1:(length(best_vars) - 6)]
best_vars <- c(best_vars, "WeekStatus", "Day_of_week")

train_set <- train_df[, c("Appliances", best_vars)]
test_set <- test_df[, c("Appliances", best_vars)]

train_set$WeekStatus <- as.factor(train_set$WeekStatus)
train_set$Day_of_week <- as.factor(train_set$Day_of_week)
test_set$WeekStatus <- as.factor(test_set$WeekStatus)
test_set$Day_of_week <- as.factor(test_set$Day_of_week)
```


Linear Model with 10-fold cross-validation
```{r}
library(ggplot2)

k <- 10
folds <- cut(seq(1, nrow(test_set)), breaks = k, labels = FALSE)

rss_values <- numeric(k)

# cross-validation
for(i in 1:k) {
  # Split the data into training and testing sets
  test_indices <- which(folds == i, arr.ind = TRUE)
  train_indices <- setdiff(seq_len(nrow(test_set)), test_indices)
  train_data <- test_set[train_indices, ]
  test_data <- test_set[test_indices, ]

  model <- lm(Appliances ~ ., data = train_data)

  predicted <- predict(model, test_data)
  rss <- sum((test_data$Appliances - predicted)^2)
  rss_values[i] <- rss
  
  cat("Fold =", i)
  cat("\nThe associated residual sum of squares is,", rss, "\n\n")
}

# plot the RSS values for each fold
plot(1:k, rss_values, xlab = "Fold", ylab = "RSS", type = "b", pch = 19, col = "blue")
```
```{r}
## get a summary of the linear model test on test_set
model <- lm(Appliances ~ ., data = test_set)
summary(model)
```
As can be seen, RH_out and T5 are not statistically significant variables, but every othr variable does seem to be pretty statistically significant to our model. We can see that the variable with the largest slope is based on what the day of the week is and also based on RH1 and T2 which both correspond to humidity in the kitchen and the temperature in the living room. 





